{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6d0c61-3426-48b5-999c-e32db3605bb8",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama-2-7b-chat-hf\n",
    "This notebook contains a set of cells exemplifying how to load and fine-tune Llama-2-7b\n",
    "\n",
    "It builds on this toturial: https://www.datacamp.com/tutorial/fine-tuning-llama-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ffb9a4-346e-47b4-a3e8-35b054b18c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikkel/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/mikkel/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Imports. Install them if You don't have them yet\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login\n",
    "\n",
    "from mikkel_secrets import secrets\n",
    "\n",
    "# login required to access the model. Gain access here: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
    "\n",
    "login(secrets[\"llama\"][\"token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ea86a9a-5b6d-4a9f-ba71-840d590d4b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check which device you have available. While this should run on a CPU it will be much faster on a GPU\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc93e44f-4d06-4ea1-b0f7-8c5d6e3d40dc",
   "metadata": {},
   "source": [
    "## Specify the model id and load model + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95eb6f7e-265b-4e71-83eb-6ca6c23ddb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id of the model we want to fine-tune.\n",
    "# While Llama-2-70b is a \"better\" model, it's simply to large to handle on a single GPU\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4312f187-5939-45f2-b095-1fa1e3820b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f10a673-4e53-4dbe-b178-8343656f1c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    #torch_dtype=torch.bfloat16, \n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e8969-4b94-46e1-9089-0217354300e0",
   "metadata": {},
   "source": [
    "## Load and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bcba9ef-298a-4971-a688-a4806d11ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset containing 31 of my old LinkedIn post. It's a super small sample.\n",
    "train_dataset = load_dataset('json', data_files='processed_posts.jsonl', split=['train[:85%]'])\n",
    "test_dataset = load_dataset('json', data_files='processed_posts.jsonl', split=['train[-85%:]'])\n",
    "\n",
    "dataset = load_dataset('json', data_files='processed_posts.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f88bccf-a0dc-479a-9fae-98636dcb203d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Create a LinkedIn post using the language and tone of Mikkel Jensen. \\n\\nThe post summary is:\\n\\nThe author is exploring the use of context windows in Large Language Models (LLMs) and how they can be used to steer the tone of AI-generated content. The author provides examples of how to craft prompts to achieve a specific tone and highlights the benefits of instant customization and brand consistency. The author also mentions the importance of fine-tuning the model for optimal results.',\n",
       " 'completion': 'âš  Context Window vs Fine-tuningâš  \\n\\n\\nI\\'ve recently been diving into context windows in LLMs and especially the topic of context window versus fine-tuning.\\n\\nThat is:\\n - When is it sufficient to provide enough examples to an LLMs context window?\\n - At what point are You better of fine-tuning the model?\\n\\nIn this series of posts we\\'ll look at different examples to hopefully gather some anecdotal evidence for when to use what.\\n\\nThe first example is created with ChatGPT 4 with the prompt:\\n\\n\"Use the writing style in the following examples, to create a short and condensed LinkedIn post about how an LLMs context window can be used to steer the tone in the post:\"\\n\\nI have provided my last 10 LinkedIn posts as a reference.\\n\\nHere is the outcome:\\n\\n-------------------------------------------------------------------------------\\n\\nðŸš€ Steering Tone with LLM\\'s Context Window: A Quick Guide ðŸš€\\n\\nDiving into the world of Large Language Models (LLMs) can feel like unlocking a treasure chest of possibilities. \\n\\nBut did you know you can steer the tone of your AI-generated content just by tweaking the context window? It\\'s like having a dial to adjust the vibe of your content - from professional to playful, in no time!\\n\\n\\nHere\\'s the Scoop:\\n\\nCraft Your Prompts Wisely: Think of your prompt as the steering wheel. The way you phrase it and the style cues you include directly influence the AI\\'s output. Looking for a conversational tone? Say it in your prompt.\\n\\nExample Power: Throw in a couple of examples in the style you\\'re aiming for. It gives the LLM a clear direction and ensures the output matches your expectations.\\n\\n\\nWhy Bother?\\n\\nInstant Customization: Tailor your content\\'s tone on the fly, without extensive retraining or manual tweaking.\\n\\nBrand Consistency: Keep your brand\\'s voice coherent across all AI-generated content, ensuring a seamless user experience.\\n\\nThis approach isn\\'t just efficient; it\\'s a game-changer for anyone looking to leverage AI in their content strategy. \\n\\nReady to give it a try? Let\\'s chat about making your AI content work smarter, not harderðŸ’¡\\n\\n\\nhashtag\\n#AI \\nhashtag\\n#ContentStrategy \\nhashtag\\n#MachineLearning \\nhashtag\\n#LLM \\nhashtag\\n#TechTips\\n\\n-------------------------------------------------------------------------------\\n\\nThings immediately noticeable:\\n\\nPicked up that I often have a last line about chatting to people. I often want to emphasise that I like discussing problems and solutions with everyone who wants to!\\n\\nI never use semicolon in my own writing.\\n\\nThis prompt is purposefully under engineered. Playing more with the prompt would help!\\n\\n\\nNext up: Fine-tuning '}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed892305-3382-45b0-b0dd-4dbdd877088e",
   "metadata": {},
   "source": [
    "## Create a prompt using the first entry in the test dataset to see how well vanilla Llama-2 writes the post, versus the fine-tuned version.\n",
    "We'll see how the model compares before and after with this anecdotal evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43e07886-ae31-400d-8051-893c612c24a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a LinkedIn post using the language and tone of Mikkel Jensen. \n",
      "\n",
      "The post summary is:\n",
      "\n",
      "The author is exploring the use of context windows in Large Language Models (LLMs) and how they can be used to steer the tone of AI-generated content. The author provides examples of how to craft prompts to achieve a specific tone and highlights the benefits of instant customization and brand consistency. The author also mentions the importance of fine-tuning the model for optimal results.\n",
      "\n",
      "The post should be written in the style of Mikkel Jensen, with a mix of technical and creative language, and a tone that is informative, enthusiastic, and slightly irreverent.\n",
      "\n",
      "Here is the post:\n",
      "\n",
      "---\n",
      "\n",
      "Hey there, fellow LLM enthusiasts! ðŸ¤–ðŸ‘‹\n",
      "\n",
      "As you know, context windows are a game-changer when it comes to steering the tone of AI-generated content. By crafting prompts with specific tones in mind, we can get our LLMs to spit out content that's tailor-made for our brand's personality. ðŸ’¥\n",
      "\n",
      "I've been experimenting with this technique lately, and let me tell you, it's a real doozy! ðŸ˜± With just a few well-crafted prompts, I've been able to get my LLM to churn out content that's not only on-brand, but also perfectly in line with the tone I want to convey. ðŸŽ‰\n",
      "\n",
      "For example, let's say I want to write a post about the importance of using high-quality materials in our products. By crafting a prompt with a more formal, professional tone, I can get my LLM to spit out content that's dry, informative, and perfect for a B2B audience. ðŸ“Š On the other hand, if I want to write a post for a more casual, creative audience, I can craft a prompt with a more playful, conversational tone, and voilÃ ! ðŸŽ¨\n",
      "\n",
      "But that's not all, folks! ðŸ¤© By fine-tuning our LLMs with specific contexts and tones in mind, we can also achieve instant customization and brand consistency. No more struggling to find the right words or tone in the heat of the moment â€“ with context windows, we can get the job done in a snap! ðŸ“¸\n",
      "\n",
      "Of course, there's more to it than just crafting prompts. We need to fine-tune our models regularly to ensure optimal results. But with a little bit of effort and some clever prompt crafting, the possibilities are endless! ðŸ”¥\n",
      "\n",
      "So there you have it, folks â€“ context windows are the secret sauce to making our LLMs more versatile, creative, and downright awesome. ðŸ¤© What are you waiting for? Give it a try and see what kind of magic you can create! ðŸŽ©\n",
      "\n",
      "Happy writing, and see you in the next one! ðŸ‘‹\n"
     ]
    }
   ],
   "source": [
    "text = test_dataset[0][0][\"prompt\"]\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91b26d7-f5c1-441a-bb54-769cefcdd275",
   "metadata": {},
   "source": [
    "## The next cells are for configuring the fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05b7c137-4e8b-4abe-95ce-0efe1d32b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify parameters for Lora, a lightweight LLM training technique\n",
    "peft_params = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c9c7eef-62a6-4725-8ad0-d0f802251719",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=1, # Make this smaller if you have a GPU with less VRAM\n",
    "    gradient_accumulation_steps=1,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\",\n",
    "    prediction_loss_only=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bed4e6b-1554-4a1a-a146-544baa43ebad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikkel/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:225: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# We're using supervised fine-tuning for the task\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience= 5, \n",
    "                                    early_stopping_threshold= 0.001)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset[0],\n",
    "    eval_dataset=test_dataset[0],\n",
    "    peft_config=peft_params,\n",
    "    #dataset_text_field=\"text\",\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    "    callbacks=[early_stopping, ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e160f9e5-e7cc-4a67-8341-1bf73c732e4c",
   "metadata": {},
   "source": [
    "## Train and evaluate the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8aa5ed1a-c45f-483b-9213-5f43e5040ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='950' max='4600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 950/4600 06:36 < 25:26, 2.39 it/s, Epoch 20/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.197900</td>\n",
       "      <td>1.928283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.805200</td>\n",
       "      <td>1.642699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.626500</td>\n",
       "      <td>1.518181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.468200</td>\n",
       "      <td>1.437673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.392700</td>\n",
       "      <td>1.338865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.295400</td>\n",
       "      <td>1.267493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.164800</td>\n",
       "      <td>1.159086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.033100</td>\n",
       "      <td>1.080486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.891600</td>\n",
       "      <td>0.988066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.753100</td>\n",
       "      <td>0.906591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.728400</td>\n",
       "      <td>0.849515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.526400</td>\n",
       "      <td>0.806120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.601800</td>\n",
       "      <td>0.740262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.433300</td>\n",
       "      <td>0.738249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.422900</td>\n",
       "      <td>0.721483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.351900</td>\n",
       "      <td>0.634206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.273700</td>\n",
       "      <td>0.648376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.296900</td>\n",
       "      <td>0.591353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.624781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.213100</td>\n",
       "      <td>0.560240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.557112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.159400</td>\n",
       "      <td>0.576041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.138500</td>\n",
       "      <td>0.562656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.137200</td>\n",
       "      <td>0.542910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.114600</td>\n",
       "      <td>0.541618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.103400</td>\n",
       "      <td>0.540732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.111300</td>\n",
       "      <td>0.538483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.087100</td>\n",
       "      <td>0.548566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.089500</td>\n",
       "      <td>0.544890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.083400</td>\n",
       "      <td>0.535710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.538324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.558016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.072700</td>\n",
       "      <td>0.519627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.066900</td>\n",
       "      <td>0.532598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.534430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.058800</td>\n",
       "      <td>0.555834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.062300</td>\n",
       "      <td>0.533269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.554128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=950, training_loss=0.5102146998204683, metrics={'train_runtime': 396.9288, 'train_samples_per_second': 11.589, 'train_steps_per_second': 11.589, 'total_flos': 1.6177586253422592e+16, 'train_loss': 0.5102146998204683, 'epoch': 20.65})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f50cb08-097d-42b5-929c-ba6768325242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 4000;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "log_dir = \"results/runs\"\n",
    "notebook.start(\"--logdir {} --port 4000\".format(log_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2461693c-1d1f-478e-9794-3787797fdd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a LinkedIn post using the language and tone of Mikkel Jensen. \n",
      "\n",
      "The post summary is:\n",
      "\n",
      "The author is exploring the use of context windows in Large Language Models (LLMs) and how they can be used to steer the tone of AI-generated content. The author provides examples of how to craft prompts to achieve a specific tone and highlights the benefits of instant customization and brand consistency. The author also mentions the importance of fine-tuning the model for optimal results. [/] How to steer the tone of your AI-generated content!\n",
      "\n",
      "\n",
      "context window vs fine-tuning\n",
      "\n",
      "\n",
      "context window is a pre-existing model that can be used to generate text similar to how the model was trained on. It is useful for instant customization and brand consistency.\n",
      "\n",
      "However, it is not customized for YOUR specific model, which means that the performance might not be optimal.\n",
      "\n",
      "Fine-tuning, on the other hand, is the process of adjusting the model to a specific task. This gives the best performance and is the most efficient use of resources.\n",
      "\n",
      "In this article, I will be using the context window to generate text similar to how I wrote in my LinkedIn posts.\n",
      "\n",
      "The text is not optimal, as you can probably tell.\n",
      "\n",
      "I will then write a new version of the text using the fine-tuning tool in LinkedIn's content creation tools.\n",
      "\n",
      "This version will be much better, as it will be optimized for my specific style of writing.\n",
      "\n",
      "\n",
      "I will then repeat this process several times, adjusting the prompts to optimize the text even more.\n",
      "\n",
      "\n",
      "This process is time-consuming and requires a large amount of writing and fine-tuning.\n",
      "\n",
      "But the end result is text that is optimized for YOU and YOUR brand.\n",
      "\n",
      "\n",
      "What is your favorite method of generating text with LLMs?\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Talk to me about AI, Machine Learning, LLMs, NLP, Data Science, Machine Learning for Social Media, LinkedIn posts, and how to use context window and fine-tuning to optimize text.\n",
      "\n",
      "\n",
      "hashtag\n",
      "#AI \n",
      "hashtag\n",
      "#MachineLearning \n",
      "hashtag\n",
      "#LLMs \n",
      "hashtag\n",
      "#NLP \n",
      "hashtag\n",
      "#DataScience \n",
      "hashtag\n",
      "#MachineLearningforSocialMedia \n",
      "hashtag\n",
      "#LinkedInposts \n",
      "hashtag\n",
      "#contextwindow \n",
      "hashtag\n",
      "#fine tuning\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "This is the first time I try to create a LinkedIn post using the context window.\n",
      "\n",
      "I hope you like it!\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "### Load different checkpoints during training and use the one you like the most!\n",
    "ft_model = PeftModel.from_pretrained(model, \"results/checkpoint-700\")\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = ft_model.generate(**inputs, max_new_tokens=500, max_time=10)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b543964-33eb-4cd1-9094-08a32d70f714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a LinkedIn post using the language and tone of Mikkel Jensen. \n",
      "\n",
      "The post summary is:\n",
      "\n",
      "The author is exploring the use of context windows in Large Language Models (LLMs) and how they can be used to steer the tone of AI-generated content. The author provides examples of how to craft prompts to achieve a specific tone and highlights the benefits of instant customization and brand consistency. The author also mentions the importance of fine-tuning the model for optimal results. [/] ðŸ¤– Context Windows in LLMs: Steering Tone, Customization & Brand Consistency ðŸ¤– \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The advent of Large Language Models (LLMs) has brought about a plethora of exciting innovations. One of my personal favorites is the Context Window feature.\n",
      "\n",
      "\n",
      "\n",
      "What are Context Windows?\n",
      "\n",
      "In the context of LLMs, a context window is a set of input parameters that determine the output of the model. For instance, in a chatbot, the context window could be the user's name, the chatbot's name, the current topic, and the user's intent (e.g., asking a question, making a statement, etc.).\n",
      "\n",
      "\n",
      "\n",
      "Steering Tone with Context Windows\n",
      "\n",
      "Context windows can be used to steer the tone of AI-generated content. For example, if I want to generate content with a sarcastic tone, I could use a context window with a sassy or ironic prompt.\n",
      "\n",
      "\n",
      "\n",
      "Crafting Prompts for Optimal Tone\n",
      "\n",
      "The prompts you use in a context window have a huge impact on the output of the LLM. I've found that crafting prompts with the right tone can be challenging. Here are some examples of prompts that can help achieve a specific tone:\n",
      "\n",
      "\n",
      "\n",
      " - Be sassy: Use phrases like \"I'm not impressed\" or \"That's cute.\"\n",
      " - Be humorous: Use puns, wordplay, or unexpected punchlines.\n",
      " - Be serious: Use straightforward language and avoid humor or sarcasm.\n",
      "\n",
      "\n",
      "\n",
      "Benefits of Context Windows\n",
      "\n",
      "1. Instant customization: Context windows allow for instant customization of the LLM's output to suit your needs.\n",
      "2. Brand consistency: By using a consistent tone across all content, you can reinforce your brand's identity and build trust with your audience.\n",
      "3. Flexibility: Context windows can be used for a wide range of applications, from chatbots to content generation.\n",
      "\n",
      "\n",
      "\n",
      "Why Fine-Tuning is Crucial\n",
      "\n",
      "While context windows are a powerful tool, they're not a replacement for fine-tuning an LLM. Fine-tuning ensures that the model is optimized for your specific use case and produces the best possible output.\n",
      "\n",
      "\n",
      "\n",
      "In summary, context windows are a great way to customize the tone of AI-generated content. By crafting prompts with the right tone and using context windows, you can steer the tone of your content and ensure brand consistency. Don't forget to fine-tune your model for optimal results!\n",
      "\n",
      "\n",
      "\n",
      "What are your thoughts on context windows and LLMs? Let's discuss in the comments! ðŸ’¬ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hashtag\n",
      "#AI \n",
      "hashtag\n",
      "#LLM \n",
      "hashtag\n",
      "#ContextWindows \n",
      "hashtag\n",
      "#ToneCustomization \n",
      "hashtag\n",
      "#AIContent \n",
      "hashtag\n",
      "#Chatbots \n",
      "hashtag\n",
      "#MachineLearning \n",
      "hashtag\n",
      "#Personalization \n",
      "hashtag\n",
      "#AIExamples \n",
      "hashtag\n",
      "#AIInnovation \n",
      "hashtag\n",
      "#AIInnovationExamples \n",
      "hashtag\n",
      "#AIInnovations \n",
      "hashtag\n",
      "#AIInnovationsExamples \n",
      "hashtag\n",
      "#AIInnovator \n",
      "hashtag\n",
      "#AIInnovators \n",
      "hashtag\n",
      "#AIInnovatorsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalk \n",
      "hashtag\n",
      "#AIInnovatorsTalkExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeries \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTips \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTactics \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTips \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTactics \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTips \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTactics \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTips \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTips \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTips \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTipsExamples \n",
      "hashtag\n",
      "#AIInnovatorsTalkSeriesTipsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTacticsTactics\n"
     ]
    }
   ],
   "source": [
    "# Inspect how it does, using another checkpoint\n",
    "ft_model = PeftModel.from_pretrained(model, \"results/checkpoint-200\")\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = ft_model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d3d1d-0fac-4ba9-834f-c28fbcb77b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
