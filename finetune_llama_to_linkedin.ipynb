{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6d0c61-3426-48b5-999c-e32db3605bb8",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama-2-7b-chat-hf\n",
    "This notebook contains a set of cells exemplifying how to load and fine-tune Llama-2-7b\n",
    "\n",
    "It builds on this toturial: https://www.datacamp.com/tutorial/fine-tuning-llama-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ffb9a4-346e-47b4-a3e8-35b054b18c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikkel/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/mikkel/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Imports. Install them if You don't have them yet\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login\n",
    "\n",
    "from mikkel_secrets import secrets\n",
    "\n",
    "# login required to access the model. Gain access here: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
    "\n",
    "login(secrets[\"llama\"][\"token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ea86a9a-5b6d-4a9f-ba71-840d590d4b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check which device you have available. While this should run on a CPU it will be much faster on a GPU\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc93e44f-4d06-4ea1-b0f7-8c5d6e3d40dc",
   "metadata": {},
   "source": [
    "## Specify the model id and load model + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95eb6f7e-265b-4e71-83eb-6ca6c23ddb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id of the model we want to fine-tune.\n",
    "# While Llama-2-70b is a \"better\" model, it's simply to large to handle on a single GPU\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4312f187-5939-45f2-b095-1fa1e3820b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f10a673-4e53-4dbe-b178-8343656f1c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:10<00:00,  5.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    #torch_dtype=torch.bfloat16, \n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed892305-3382-45b0-b0dd-4dbdd877088e",
   "metadata": {},
   "source": [
    "## Create a promt that we will use twice. One time with the vanilla model and once after fine-tuning\n",
    "We'll see how the model compares before and after with this anecdotal evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e07886-ae31-400d-8051-893c612c24a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Create a LinkedIn post using the language and tone of Mikkel Jensen. \n",
      "The post is about how I fine-tuned LLama-2 to write in my language and should start with the title:\n",
      "How I fine-tuned an LLM to write like me!\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "How I fine-tuned an LLM to write like me!\n",
      "\n",
      "Hey there, fellow language enthusiasts! 🤩 I'm thrilled to share with you how I fine-tuned an LLM (Large Language Model) to write like me! 💡\n",
      "\n",
      "As some of you may know, I'm a big fan of language and have been experimenting with various LLMs to see how they can help me improve my writing skills. And let me tell you, it's been a wild ride! 🚀\n",
      "\n",
      "At first, I thought it would be easy to just fine-tune an LLM to mimic my writing style. But, oh boy, was I wrong! 😳 It turned out to be a complex and time-consuming process, but the end result was totally worth it.\n",
      "\n",
      "Here are the steps I took to fine-tune LLama-2 (my LLM of choice):\n",
      "\n",
      "1. **Data augmentation:** I fed LLama-2 a diverse range of texts, including my own writing, to help it learn the nuances of my language. This included blog posts, articles, and even some of my old school essays. 📚\n",
      "2. **Adversarial training:** I trained LLama-2 on a dataset of my own writing, using a technique called adversarial training. This involved feeding LLama-2 both my original texts and slightly modified versions, so it could learn to recognize and generate text that's similar to mine. 🔥\n",
      "3. **Prompt engineering:** I crafted custom prompts for LLama-2, tailored to specific topics and styles. This allowed me to test how well LLama-2 could generate text in different areas, such as technical writing or creative writing. 📝\n",
      "4. **Evaluation and refinement:** I continuously evaluated LLama-2's performance on a variety of tasks, such as text generation, language translation, and even creative writing challenges. This helped me identify areas where LLama-2 needed improvement and refine its training accordingly. 📈\n",
      "\n",
      "The result of all this fine-tuning? LLama-2 can now generate text that's eerily similar to mine! 😲 I'm not kidding – sometimes I forget that it's not actually me writing. 😅 Of course, there are still some limitations to LLama-2's capabilities, but I'm excited to see how far it can take me in terms of creative writing and language exploration. 🌟\n",
      "\n",
      "So, if you're curious about how to fine-tune an LLM for your own writing style, I hope my experience inspires you to give it a try! 💡 Just remember, it takes time, patience, and a good dose of creativity to get it right. 😉\n",
      "\n",
      "What are your thoughts on LLMs and their potential for creative writing? Let me know in the comments! 💬\n",
      "\n",
      "---\n",
      "\n",
      "This post is written in a playful and enthusiastic tone, with a touch of humor and creativity. Mikkel Jensen is known for his love of language and his ability to experiment with different writing styles, so the post reflects his excitement and curiosity about the potential of LLMs for creative writing. The language is informal and conversational, with a focus on the technical aspects of fine-tuning an LLM. The post also includes some humor and personality, with a touch of sarcasm and irony, to keep the reader engaged and entertained.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "Create a LinkedIn post using the language and tone of Mikkel Jensen. \n",
    "The post is about how I fine-tuned LLama-2 to write in my language and should start with the title:\n",
    "How I fine-tuned an LLM to write like me!\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91b26d7-f5c1-441a-bb54-769cefcdd275",
   "metadata": {},
   "source": [
    "## The next cells are for loading the dataset and configuring the fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ca0d5e1-76ad-493d-b6d9-e84ab1a0efa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset containing 31 of my old LinkedIn post. It's a super small sample.\n",
    "train_dataset = load_dataset('json', data_files='data/linkedin_posts.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc65712b-d781-48eb-94a2-daaa10c644a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['prompt', 'completion']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f2e6d44-4628-4e55-aaef-740c1d05f7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'🤖 ML & AI use cases You can utilize right now 🤖\\n\\n\\nEmploy a Chatbot\\n\\nDo you have a web page containing a vast amount of information?\\n Getting questions about how to find and use specific features?\\nIs the FAQ page not quite cutting it?\\nConsider using a chatbot to act as a virtual assistant. Chatbots can answer a lot of the easier questions and offload customer support personnel.\\n\\nA chatbot I have personally found helpful is the one Visma Dinero has implemented. It is super useful to have some help at hand for someone like me, who does not know that much about bookkeeping.\\n\\n\\nCreate a Recommendation System\\n\\nRecommendation systems are widely used in online shops and on streaming services such as Netflix and Spotify.\\nThe algorithms ensure that a movie or song that You are likely to also like is recommended.\\nGood for optimizing revenue per customer as related items are displayed in addition to what someone is looking to buy already.\\n\\n\\nCustomer Profiling\\n\\nKnowing Your ideal customer profile greatly improves the understanding of how to market a product correctly.\\nHow old are they? What do they do? Are they individuals or corporate? Thorough analysis of the customer profile is a key part of targeted ads and ensures reaching out to the right people when selling.\\n\\n\\nOutlier Detection\\n\\nAre You unsure whether an event is a statistical fluke or should be acted upon?\\nUnderstanding this can be crucial in decision making.\\nOutlier detection is often used in finance to identify fraudulent transactions or monitor stock market anomalies.\\n\\n\\n\\nUnsure of how to get going with any of the above?\\n\\nLet us have an informal chat.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the first entry in the dataset\n",
    "str(train_dataset[\"train\"][0][\"completion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4838c347-4ebe-4617-9ac4-49541f019d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary of the following text delimited by triple backquotes. The summary should have a length of 50 tokens maximum.\n",
      "    ```🤖 ML & AI use cases You can utilize right now 🤖\n",
      "\n",
      "\n",
      "Employ a Chatbot\n",
      "\n",
      "Do you have a web page containing a vast amount of information?\n",
      " Getting questions about how to find and use specific features?\n",
      "Is the FAQ page not quite cutting it?\n",
      "Consider using a chatbot to act as a virtual assistant. Chatbots can answer a lot of the easier questions and offload customer support personnel.\n",
      "\n",
      "A chatbot I have personally found helpful is the one Visma Dinero has implemented. It is super useful to have some help at hand for someone like me, who does not know that much about bookkeeping.\n",
      "\n",
      "\n",
      "Create a Recommendation System\n",
      "\n",
      "Recommendation systems are widely used in online shops and on streaming services such as Netflix and Spotify.\n",
      "The algorithms ensure that a movie or song that You are likely to also like is recommended.\n",
      "Good for optimizing revenue per customer as related items are displayed in addition to what someone is looking to buy already.\n",
      "\n",
      "\n",
      "Customer Profiling\n",
      "\n",
      "Knowing Your ideal customer profile greatly improves the understanding of how to market a product correctly.\n",
      "How old are they? What do they do? Are they individuals or corporate? Thorough analysis of the customer profile is a key part of targeted ads and ensures reaching out to the right people when selling.\n",
      "\n",
      "\n",
      "Outlier Detection\n",
      "\n",
      "Are You unsure whether an event is a statistical fluke or should be acted upon?\n",
      "Understanding this can be crucial in decision making.\n",
      "Outlier detection is often used in finance to identify fraudulent transactions or monitor stock market anomalies.\n",
      "\n",
      "\n",
      "\n",
      "Unsure of how to get going with any of the above?\n",
      "\n",
      "Let us have an informal chat.```\n",
      "🤖 The text discusses four ML & AI use cases: \n",
      "\n",
      "1. Employing a chatbot to answer customer questions and offload support personnel.\n",
      "2. Creating a recommendation system to optimize revenue per customer.\n",
      "3. Conducting customer profiling to improve marketing efforts.\n",
      "4. Detecting outliers in data to make informed decisions.\n",
      "\n",
      "The text also mentions the potential to discuss how to implement these use cases further.\n"
     ]
    }
   ],
   "source": [
    "to_sum = \"\"\"Write a concise summary of the following text delimited by triple backquotes. The summary should have a length of 50 tokens maximum.\n",
    "    ```\"\"\" + str(train_dataset[\"train\"][0][\"completion\"]) + \"\"\"```\"\"\"\n",
    "inputs = tokenizer(to_sum, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa1aec56-80c6-4992-b1f6-82d0dea48dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05b7c137-4e8b-4abe-95ce-0efe1d32b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify parameters for Lora, a lightweight LLM training technique\n",
    "peft_params = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c9c7eef-62a6-4725-8ad0-d0f802251719",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=2, # Make this smaller if you have a GPU with less VRAM\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=int(len(train_dataset[\"train\"]) / 2),\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\",\n",
    "    prediction_loss_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bed4e6b-1554-4a1a-a146-544baa43ebad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikkel/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:225: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map: 100%|█████████████████████████████| 31/31 [00:00<00:00, 1287.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# We're using supervised fine-tuning for the task\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset[\"train\"],\n",
    "    peft_config=peft_params,\n",
    "    #dataset_text_field=\"text\",\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e160f9e5-e7cc-4a67-8341-1bf73c732e4c",
   "metadata": {},
   "source": [
    "## Train and evaluate the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8aa5ed1a-c45f-483b-9213-5f43e5040ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 00:53, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.486500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.941000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.603200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.971300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.645500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=160, training_loss=1.4129061222076416, metrics={'train_runtime': 54.4122, 'train_samples_per_second': 5.697, 'train_steps_per_second': 2.941, 'total_flos': 4792682994057216.0, 'train_loss': 1.4129061222076416, 'epoch': 10.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed4a46f3-c02c-4b36-80b6-c6d6cdb3a2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Create a LinkedIn post using the language and tone of Mikkel Jensen. \n",
      "The post is about how I fine-tune LLama-2 to write in my language and should start with the title: How I fine-tuned an LLM to write like me!\n",
      "\n",
      "\n",
      "How I fine-tuned an LLM to write like me!\n",
      "\n",
      "The latest trend in AI is Large Language Models (LLMs) and their ability to generate text, images and even code.\n",
      "\n",
      "I wanted to try one out and see how it would perform.\n",
      "\n",
      "I started by using the excellent and open-source tool Hugging Face’s Transformers For NLP, which allows you to browse, download and use pre-trained models.\n",
      "\n",
      "I downloaded the most popular model - LLaMA - and used it to generate some text. It was pretty good, but not great. It lacked the nuance and personality that I try to bring to my posts.\n",
      "\n",
      "So I decided to fine-tune it!\n",
      "\n",
      "Fine-tuning an LLM is a simple process. You need to have a pre-trained model and some Python code.\n",
      "\n",
      "Here’s the code I used:\n",
      "\n",
      "https://lnkd.in/eY_mXP5Z\n",
      "\n",
      "\n",
      "I also added a simple counter to the model, so it can keep track of how many sentences it has written.\n",
      "\n",
      "\n",
      "And here’s the result:\n",
      "https://lnkd.in/eY_mXP5Z\n",
      "\n",
      "\n",
      "It’s still not as good as my writing, but it’s pretty darn close!\n",
      "\n",
      "I’m sure that with more fine-tuning it will get even better.\n",
      "\n",
      "What do you think? Have you tried anything similar?\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Talk to me about AI, NLP, Machine Learning and Data Science!\n",
      "\n",
      "👨‍💻 I’m Mikkel Jensen, a Data Scientist and AI consultant.\n",
      "\n",
      "hashtag#AI hashtag#NLP hashtag#MachineLearning hashtag#DataScience \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load different checkpoints during training and use the one you like the most!\n",
    "ft_model = PeftModel.from_pretrained(model, \"results/checkpoint-105\")\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = ft_model.generate(**inputs, max_new_tokens=500, max_time=10)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b543964-33eb-4cd1-9094-08a32d70f714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a LinkedIn post using the language and tone of Mikkel Jensen. The post is about how context window compares to fine tuning LLMs, and the advantages and disadvantages of both. [/INST] How is the performance of a model that I'm using for generating text between fine tuning a language model (LWM) model and a model with a single prediction that I'll be able to estimate the probability of each character in a sentence?\n",
      "\n",
      "The model I’m using has around 2.5 million parameters and is estimated from around 300 characters worth of data.\n",
      "\n",
      "We can estimate the model in around 10 minutes using a GPU.\n",
      "\n",
      "The model has around 70% accuracy on the test set.\n",
      "\n",
      "Fine tuning a LWM model to have around the same accuracy would take around 30 minutes and the model would have around 30% additional parameters.\n",
      "\n",
      "Fate tuning a LWM model takes around 3 minutes per iteration.\n",
      "\n",
      "The model has around 70% accuracy on the test set.\n",
      "\n",
      "The main advantage of a single prediction model is that it is faster to estimate and has fewer parameters. The disadvantage is that it has a lower accuracy.\n",
      "\n",
      "The main advantage of fine tuning a LWM model is that it has a higher accuracy. The disadvantage is that it takes more time to estimate and has more parameters.\n",
      "\n",
      "I think that a single prediction model is a good choice for many applications where accuracy is not the most important factor, for example generating text for chatbots.\n",
      "Fine tuning a LWM model is a good choice for applications where accuracy is important and where a lot of data is available, for example translating text.\n",
      "\n",
      "What do you think? 🤔📝\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Talking machine learning, natural language processing, and everything related!\n",
      "\n",
      "hashtag#machinelearning hashtag#python hashtag#data hashtag#fate hashtag#opensource \n",
      "\n",
      "Feel free to reach out if you have any questions or need help with a project!\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "A LinkedIn post created with the language and tone of Mikkel Jensen. The post compares a single prediction model for generating text with fine tuning a language model and their pros and cons. The single prediction model is faster to estimate with fewer parameters, but has lower accuracy. Fine tuning a language model has a higher accuracy, but takes more time to estimate. \n",
      "\n",
      "The post highlights that the single prediction model is useful for applications where accuracy is not the primary concern and where speed is important. Fine tuning a language model is useful where accuracy is important and a lot of data is available.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MSMSPAGAA,MSMS,MS,MS,MS,MS,MSMS,MSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSPAMSMSMSPAGMSMSPAPAMSMSMSMSPAPAMSMSPAPAMSMSPAPAMSMSPAMSPAPAMSMSPAMSPAMSMSMSMSPAPAMSMSMSMSMSMSPAMSMSMSMSPAMSMSPAMSMSMSPAMSMSMSMSPAMSMSMSMSMSMSPAMSMSMSPAMSMSMSMSPAMSMSMSMSPAMSMSMSMSMSMSMSPAMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMS\n"
     ]
    }
   ],
   "source": [
    "# Inspect how it does, when having seen all data ~10 times\n",
    "ft_model = PeftModel.from_pretrained(model, \"results/checkpoint-150\")\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = ft_model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
